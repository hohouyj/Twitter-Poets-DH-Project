{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Import Libaries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "#from textblob import TextBlob\n",
    "import langid\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "source": [
    "Read CSV with pandas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "trump_tweets = pd.read_csv('hashtag_donaldtrump.csv',lineterminator='\\n')\n",
    "trump_tweets = trump_tweets.drop(   ['country'\n",
    "                                    ,'continent'\n",
    "                                    ,'state'\n",
    "                                    ,'user_name'\n",
    "                                    ,'user_id'\n",
    "                                    ,'user_screen_name'\n",
    "                                    ,'tweet_id'\n",
    "                                    ,'lat'\n",
    "                                    ,'long'\n",
    "                                    ,'city'\n",
    "                                    ,'state_code'\n",
    "                                    ,'source'\n",
    "                                    ,'collected_at'\n",
    "                                    ,'user_followers_count'\n",
    "                                    ,'user_location'\n",
    "                                    ,'user_description'\n",
    "                                    ,'user_join_date']\n",
    "                                    ,axis = 1)\n",
    "\n",
    "biden_tweets = pd.read_csv('hashtag_joebiden.csv',lineterminator='\\n')\n",
    "biden_tweets = biden_tweets.drop(   ['country'\n",
    "                                    ,'continent'\n",
    "                                    ,'state'\n",
    "                                    ,'user_name'\n",
    "                                    ,'user_id'\n",
    "                                    ,'user_screen_name'\n",
    "                                    ,'tweet_id'\n",
    "                                    ,'lat'\n",
    "                                    ,'long'\n",
    "                                    ,'city'\n",
    "                                    ,'state_code'\n",
    "                                    ,'source'\n",
    "                                    ,'collected_at'\n",
    "                                    ,'user_followers_count'\n",
    "                                    ,'user_location'\n",
    "                                    ,'user_description'\n",
    "                                    ,'user_join_date']\n",
    "                                    ,axis = 1)\n",
    "'''\n",
    "# print('#Trump Tweets:\\n',trump_tweets.head())\n",
    "# print(len(trump_tweets),trump_tweets.shape)\n",
    "# print(\"#Biden Tweets:\\n\",biden_tweets.head())\n",
    "# print(biden_tweets.shape)"
   ]
  },
  {
   "source": [
    "Get Tweets for dates"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#date format ddmmyyyy\n",
    "def get_tweets_dates(df,start_date,stop_date):\n",
    "    ls = []\n",
    "    start_month = int(start_date[2:4])\n",
    "    stop_month = int(stop_date[2:4])\n",
    "    start_day = int(start_date[0:2])\n",
    "    stop_day = int(stop_date[0:2])\n",
    "    gen = df.itertuples()\n",
    "    for i in gen:\n",
    "        tweet_date = i[1]\n",
    "        tweet_month = int(tweet_date[5:7])\n",
    "        tweet_day = int(tweet_date[8:10])\n",
    "        #print(tweet_month,tweet_day)\n",
    "        if tweet_month >= start_month and tweet_month <= stop_month:\n",
    "            if tweet_month == start_month and tweet_day < start_day:\n",
    "                pass\n",
    "            elif tweet_month == stop_month and tweet_day > stop_day:\n",
    "                pass\n",
    "            else:\n",
    "                ls.append(i)\n",
    "    return pd.DataFrame(ls)\n",
    "# test = get_tweets_dates(biden_tweets,'20102020','20102020')\n",
    "# print(test.iloc[0])\n",
    "# print(test.iloc[-1])"
   ]
  },
  {
   "source": [
    "Trump: oct:[15-31],nov:[1-8]: 25\n",
    "\n",
    "Biden: Oct[15-31],nov:[1-8] : 25"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_oct_dates = [str(i)+'102020' for i in range(15,32)]\n",
    "ls_nov_dates = ['0'+str(i)+'112020' for i in range(1,9)]\n",
    "ls_dates = ls_oct_dates+ls_nov_dates\n",
    "\n",
    "def get_dict_tweets_dates(ls_dates,df):\n",
    "    re_dd = {}\n",
    "    for i in ls_dates:\n",
    "        re_dd[i] = get_tweets_dates(df,i,i)\n",
    "    return re_dd\n",
    "\n",
    "#trump_tweets_by_day_dd = get_dict_tweets_dates(ls_dates,trump_tweets)\n",
    "#biden_tweets_by_day_dd = get_dict_tweets_dates(ls_dates,biden_tweets)"
   ]
  },
  {
   "source": [
    "Helper Fucntions for Cleaning Text"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "before cleaning:  ni hao mah\nafter cleaning:  ### NON-ENGLISH LANGUAGE ###\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ')\n",
    "    return text\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def remove_newline(text):\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.replace(\"\\t\", \"\")\n",
    "    return text\n",
    "\n",
    "def remove_urls(text):\n",
    "    ls = text.split()\n",
    "    re_str = ''\n",
    "    for i in ls:\n",
    "        if 'https' in i:\n",
    "            pass\n",
    "        else:\n",
    "            re_str+=' '+i\n",
    "    return re_str\n",
    "def filter_english(text):\n",
    "    test = langid.classify(text)[0]\n",
    "    # ('en', -54.41310358047485)\n",
    "    # sleep(100)\n",
    "    if test == 'en':\n",
    "        return text\n",
    "    else:\n",
    "        return '### NON-ENGLISH LANGUAGE ###'\n",
    "def clean_everything(text):\n",
    "    text = str(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_newline(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = filter_english(text)\n",
    "    return text\n",
    "\n",
    "def clean_tweet_df(df):\n",
    "    df['tweet'] = df['tweet'].progress_apply(lambda x: clean_everything(x))\n",
    "    return df\n",
    "\n",
    "def clean_tweet_df_from_dd(df_dd):\n",
    "    re_dd = {}\n",
    "    keys = df_dd.keys()\n",
    "    for i in keys:\n",
    "        df = df_dd[i]\n",
    "        re_dd[i] = clean_tweet_df(df)\n",
    "    return re_dd\n",
    "\n",
    "def remove_non_english_rows_from_df(df):\n",
    "    df = df.drop(df[df.tweet == '### NON-ENGLISH LANGUAGE ###'].index)\n",
    "    df = df.drop(df[df.tweet == 'NON-ENGLISH LANGUAGE'].index)\n",
    "    df = df.drop(df[df.tweet == ' NON-ENGLISH LANGUAGE '].index)\n",
    "    df = df.drop(df[df.tweet == ' NON-ENGLISH LANGUAGE'].index)\n",
    "    df = df.drop(df[df.tweet == 'NON-ENGLISH LANGUAGE '].index)\n",
    "    df = df.drop(df[df.tweet == '### NON ENGLISH LANGUAGE ###'].index)\n",
    "    df = df.drop(df[df.tweet == 'NON ENGLISH LANGUAGE'].index)\n",
    "    df = df.drop(df[df.tweet == ' NON ENGLISH LANGUAGE '].index)\n",
    "    df = df.drop(df[df.tweet == ' NON ENGLISH LANGUAGE'].index)\n",
    "    df = df.drop(df[df.tweet == 'NON ENGLISH LANGUAGE '].index)\n",
    "    return df\n",
    "\n",
    "print('before cleaning: ', 'ni hao mah')\n",
    "print('after cleaning: ', clean_everything('ni hao mah'))"
   ]
  },
  {
   "source": [
    "Clean Each DataFrame and save to csv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trump_tweets_by_day_dd = clean_tweet_df_from_dd(trump_tweets_by_day_dd)\n",
    "#biden_tweets_by_day_dd = clean_tweet_df_from_dd(biden_tweets_by_day_dd)\n",
    "#print('Done Cleaning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csvs_from_dd(df_dd,name):\n",
    "    keys = df_dd.keys()\n",
    "    for i in keys:\n",
    "        df = df_dd[i]\n",
    "        df.to_csv(f'tweets_by_day/{i}_{name}.csv',index = False)\n",
    "\n",
    "# save_to_csvs_from_dd(biden_tweets_by_day_dd,'biden')\n",
    "# save_to_csvs_from_dd(trump_tweets_by_day_dd,'trump')"
   ]
  },
  {
   "source": [
    "Collect data from csvs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nDone!\n2         Had to be same success as Trump s casino busi...\n3         With the COVID 19 pandemic dragging on and th...\n4         realDonaldTrump BREAKING COVID 19 OUTBREAK IN...\n6         CapehartJ This may be a rhetorical question b...\n7         Heads up AmyComeyBarrett if ClarenceThomas me...\n                               ...                        \n22976          trump2020 OperationMAGA Trump MAGA2020 maga\n22977     DonaldTrump realDonaldTrump SenTedCruz Saving...\n22979     Because the Trump Campaign are Liars and Liar...\n22980     therecount Yes we do That s what ypur lying b...\n22981     trump wants to make it illegal to cover COVID...\nName: tweet, Length: 17829, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def get_dd_from_csv_folder(foldername,name):\n",
    "    re_dd = {}\n",
    "    for i in os.listdir(foldername):\n",
    "        #print(i)\n",
    "        if i[9:-4] == name:\n",
    "            df = pd.read_csv(foldername+'/'+i)\n",
    "            #print(i[0:8])\n",
    "            re_dd[i[0:8]] = remove_non_english_rows_from_df(df)\n",
    "        else:\n",
    "            pass\n",
    "    return re_dd\n",
    "biden_tweets_by_day_dd = get_dd_from_csv_folder('tweets_by_day','biden')\n",
    "trump_tweets_by_day_dd = get_dd_from_csv_folder('tweets_by_day','trump')\n",
    "print('\\nDone!')\n",
    "print(trump_tweets_by_day_dd['27102020']['tweet'])"
   ]
  },
  {
   "source": [
    "Get the TF-IDF for the tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-838c1c9729af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mfrom_dd_to_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbiden_tweets_by_day_dd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'biden'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mfrom_dd_to_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrump_tweets_by_day_dd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'trump'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-838c1c9729af>\u001b[0m in \u001b[0;36mfrom_dd_to_tfidf\u001b[1;34m(dd, file_name)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mtest_ls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_non_english_rows_from_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mtest_ls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtest_ls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-838c1c9729af>\u001b[0m in \u001b[0;36mtf_idf\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#print(doc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtfidf_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \"\"\"\n\u001b[0;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1841\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1199\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[0;32m    220\u001b[0m                              \"unicode string.\")\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tf_idf(df):\n",
    "    doc=df['tweet'].values #df is your dataframe\n",
    "    #print(doc)\n",
    "    tf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tf.fit_transform(doc)\n",
    "    feature_names = tf.get_feature_names()\n",
    "    doc = 0\n",
    "    feature_index = tfidf_matrix[doc,:].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[doc, x] for x in feature_index])\n",
    "    ls = []\n",
    "    for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "        ls.append((w,s))\n",
    "    return ls\n",
    "\n",
    "def from_dd_to_tfidf(dd,file_name):\n",
    "    keys = dd.keys()\n",
    "    ls = []\n",
    "    for i in keys:\n",
    "        test_ls = tf_idf(remove_non_english_rows_from_df(dd[i]))\n",
    "        test_ls.insert(0,(i,i))\n",
    "        ls=ls+test_ls\n",
    "    csv_df = pd.DataFrame(ls)\n",
    "    csv_df.to_csv(f'tweets_by_day_tfidf/tfidf_{file_name}.csv',index=False)\n",
    "    return ls\n",
    "\n",
    "from_dd_to_tfidf(biden_tweets_by_day_dd,'biden')\n",
    "from_dd_to_tfidf(trump_tweets_by_day_dd,'trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_k_retweets_daily(dd,k,filename):\n",
    "    keys = dd.keys()\n",
    "    re_dd = {}\n",
    "    try:\n",
    "        os.mkdir(f'top_{k}_retweets_by_day')\n",
    "    except:\n",
    "        print('oop')\n",
    "    for i in keys:\n",
    "        df = remove_non_english_rows_from_df(dd[i])\n",
    "        top_k = df.nlargest(k,'retweet_count')\n",
    "        re_dd[i] = top_k\n",
    "        top_k.to_csv(f'top_{k}_retweets_by_day/{i}_{filename}.csv',index = False)\n",
    "    return re_dd\n",
    "\n",
    "def get_top_k_likes_daily(dd,k,filename):\n",
    "    keys = dd.keys()\n",
    "    re_dd = {}\n",
    "    try:\n",
    "        os.mkdir(f'top_{k}_likes_by_day')\n",
    "    except:\n",
    "        print('oop')\n",
    "    for i in keys:\n",
    "        df = remove_non_english_rows_from_df(dd[i])\n",
    "        top_k = df.nlargest(k,'likes')\n",
    "        re_dd[i] = top_k\n",
    "        top_k.to_csv(f'top_{k}_likes_by_day/{i}_{filename}.csv',index = False)\n",
    "    return re_dd\n",
    "\n",
    "\n",
    "\n",
    "get_top_k_retweets_daily(trump_tweets_by_day_dd,5,'trump')\n",
    "get_top_k_retweets_daily(biden_tweets_by_day_dd,5,'biden')\n",
    "get_top_k_likes_daily(trump_tweets_by_day_dd,5,'trump')\n",
    "get_top_k_likes_daily(biden_tweets_by_day_dd,5,'biden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}